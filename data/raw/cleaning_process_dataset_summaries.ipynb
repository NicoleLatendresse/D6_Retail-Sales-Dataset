{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217f427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BEFORE CLEANING\n",
      "======================================================================\n",
      "Rows, Columns: 1000, 9\n",
      "Duplicate rows (full row): 0\n",
      "\n",
      "Missing values (only columns with >0 missing):\n",
      "None\n",
      "\n",
      "Column dtypes:\n",
      "Transaction ID       int64\n",
      "Date                object\n",
      "Customer ID         object\n",
      "Gender              object\n",
      "Age                  int64\n",
      "Product Category    object\n",
      "Quantity             int64\n",
      "Price per Unit       int64\n",
      "Total Amount         int64\n",
      "dtype: object\n",
      "\n",
      "======================================================================\n",
      "AFTER CLEANING\n",
      "======================================================================\n",
      "Rows, Columns: 1000, 13\n",
      "Duplicate rows (full row): 0\n",
      "\n",
      "Missing values (only columns with >0 missing):\n",
      "None\n",
      "\n",
      "Column dtypes:\n",
      "transaction_id               int64\n",
      "date                datetime64[ns]\n",
      "customer_id                 object\n",
      "gender                    category\n",
      "age                          int64\n",
      "product_category          category\n",
      "quantity                     int64\n",
      "price_per_unit               int64\n",
      "total_amount                 int64\n",
      "year                         int32\n",
      "month                        int32\n",
      "month_name                category\n",
      "weekday                   category\n",
      "dtype: object\n",
      "\n",
      "Date range: 2023-01-01 00:00:00  ->  2024-01-01 00:00:00\n",
      "\n",
      "age stats:\n",
      "count    1000.00000\n",
      "mean       41.39200\n",
      "std        13.68143\n",
      "min        18.00000\n",
      "25%        29.00000\n",
      "50%        42.00000\n",
      "75%        53.00000\n",
      "max        64.00000\n",
      "Name: age, dtype: float64\n",
      "\n",
      "quantity stats:\n",
      "count    1000.000000\n",
      "mean        2.514000\n",
      "std         1.132734\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         3.000000\n",
      "75%         4.000000\n",
      "max         4.000000\n",
      "Name: quantity, dtype: float64\n",
      "\n",
      "price_per_unit stats:\n",
      "count    1000.000000\n",
      "mean      179.890000\n",
      "std       189.681356\n",
      "min        25.000000\n",
      "25%        30.000000\n",
      "50%        50.000000\n",
      "75%       300.000000\n",
      "max       500.000000\n",
      "Name: price_per_unit, dtype: float64\n",
      "\n",
      "total_amount stats:\n",
      "count    1000.000000\n",
      "mean      456.000000\n",
      "std       559.997632\n",
      "min        25.000000\n",
      "25%        60.000000\n",
      "50%       135.000000\n",
      "75%       900.000000\n",
      "max      2000.000000\n",
      "Name: total_amount, dtype: float64\n",
      "\n",
      "Top values for gender:\n",
      "gender\n",
      "Female    510\n",
      "Male      490\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top values for product_category:\n",
      "product_category\n",
      "Clothing       351\n",
      "Electronics    342\n",
      "Beauty         307\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "CLEANING REPORT (WHAT CHANGED)\n",
      "======================================================================\n",
      "start_rows: 1000\n",
      "start_cols: 9\n",
      "renamed_columns: 9\n",
      "category_unique_before: 3\n",
      "category_unique_after: 3\n",
      "date_parse_na_before: 0\n",
      "date_parse_na_after: 0\n",
      "age_na_before_numeric_convert: 0\n",
      "age_na_after_numeric_convert: 0\n",
      "quantity_na_before_numeric_convert: 0\n",
      "quantity_na_after_numeric_convert: 0\n",
      "price_per_unit_na_before_numeric_convert: 0\n",
      "price_per_unit_na_after_numeric_convert: 0\n",
      "total_amount_na_before_numeric_convert: 0\n",
      "total_amount_na_after_numeric_convert: 0\n",
      "rows_dropped_missing_critical: 0\n",
      "age_filled_n: 0\n",
      "rows_removed_duplicates: 0\n",
      "rows_removed_bad_age: 0\n",
      "rows_removed_bad_quantity: 0\n",
      "rows_removed_bad_price_per_unit: 0\n",
      "total_amount_mismatches_before_fix: 0\n",
      "total_amount_mismatches_after_fix: 0\n",
      "final_rows: 1000\n",
      "final_cols: 13\n",
      "rows_removed_total: 0\n",
      "\n",
      "Cleaned file saved to:\n",
      "C:\\Users\\Mary\\D6_Retail-Sales-Dataset\\data\\processed\\retail_sales_dataset_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading .csv dataset\n",
    "INPUT_PATH = r\"C:\\Users\\Mary\\D6_Retail-Sales-Dataset\\data\\raw\\retail_sales_dataset.csv\"\n",
    "df_raw = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Keeping original copy for before and after comparison\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Adding functions\n",
    "def clean_column_names(columns):\n",
    "    \"\"\"Change column names to snake_case and remove extra spaces\"\"\"\n",
    "    return (pd.Index(columns)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "    .str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "def standardize_text(series):\n",
    "    \"\"\"Remove spaces from text\"\"\"\n",
    "    return series.astype(str).str.strip()\n",
    "def summarize_df(df_in, label=\"SUMMARY\"):\n",
    "    \"\"\"Print a dataset summary: shape, dtypes, missing values, duplicates, quick stats for columns \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{label}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Rows, Columns: {df_in.shape[0]}, {df_in.shape[1]}\")\n",
    "    print(f\"Duplicate rows (full row): {df_in.duplicated().sum()}\")\n",
    "\n",
    "    # Missing values\n",
    "    missing = df_in.isna().sum().sort_values(ascending=False)\n",
    "    missing = missing[missing > 0]\n",
    "    print(\"\\nMissing values (only columns with >0 missing):\")\n",
    "    if len(missing) == 0:print(\"None\")\n",
    "    else:print(missing)\n",
    "\n",
    "    # Dtypes\n",
    "    print(\"\\nColumn dtypes:\")\n",
    "    print(df_in.dtypes)\n",
    "\n",
    "    # Checking columns where applicable\n",
    "    if \"date\" in df_in.columns:\n",
    "        dt_min = df_in[\"date\"].min() if pd.api.types.is_datetime64_any_dtype(df_in[\"date\"]) else \"not datetime yet\"\n",
    "        dt_max = df_in[\"date\"].max() if pd.api.types.is_datetime64_any_dtype(df_in[\"date\"]) else \"not datetime yet\"\n",
    "        print(f\"\\nDate range: {dt_min}  ->  {dt_max}\")\n",
    "    for c in [\"age\", \"quantity\", \"price_per_unit\", \"total_amount\"]:\n",
    "        if c in df_in.columns:\n",
    "            print(f\"\\n{c} stats:\")\n",
    "            print(df_in[c].describe())\n",
    "    for c in [\"gender\", \"product_category\"]:\n",
    "        if c in df_in.columns:\n",
    "            print(f\"\\nTop values for {c}:\")\n",
    "            print(df_in[c].value_counts(dropna=False).head(10))\n",
    "\n",
    "# Creating summary of original file for before and after comparison\n",
    "summarize_df(df, label=\"BEFORE CLEANING\")\n",
    "\n",
    "# Keeping track of before and after cleaning metrics\n",
    "report = {\"start_rows\": len(df),\"start_cols\": df.shape[1]}\n",
    "\n",
    "# Cleaning column names\n",
    "old_cols = df.columns.tolist()\n",
    "df.columns = clean_column_names(df.columns)\n",
    "new_cols = df.columns.tolist()\n",
    "report[\"renamed_columns\"] = sum([1 for o, n in zip(old_cols, new_cols) if o != n])\n",
    "\n",
    "# Standardizing text columns\n",
    "text_cols = [\"customer_id\", \"gender\", \"product_category\"]\n",
    "for col in text_cols:\n",
    "    if col in df.columns:df[col] = standardize_text(df[col])\n",
    "\n",
    "# Standardizing product category formatting\n",
    "if \"product_category\" in df.columns:\n",
    "    before_cat_unique = df[\"product_category\"].nunique(dropna=False)\n",
    "    df[\"product_category\"] = df[\"product_category\"].str.title()\n",
    "    after_cat_unique = df[\"product_category\"].nunique(dropna=False)\n",
    "    report[\"category_unique_before\"] = before_cat_unique\n",
    "    report[\"category_unique_after\"] = after_cat_unique\n",
    "\n",
    "# Adjusting data type date to datetime\n",
    "if \"date\" in df.columns:\n",
    "    before_bad_dates = df[\"date\"].isna().sum()  # still string, but count NA now\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    after_bad_dates = df[\"date\"].isna().sum()\n",
    "    report[\"date_parse_na_before\"] = before_bad_dates\n",
    "    report[\"date_parse_na_after\"] = after_bad_dates\n",
    "\n",
    "# Number conversions if applicable\n",
    "numeric_cols = [\"age\", \"quantity\", \"price_per_unit\", \"total_amount\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        before_non_numeric_na = df[col].isna().sum()\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        after_non_numeric_na = df[col].isna().sum()\n",
    "        report[f\"{col}_na_before_numeric_convert\"] = before_non_numeric_na\n",
    "        report[f\"{col}_na_after_numeric_convert\"] = after_non_numeric_na\n",
    "\n",
    "# Checking for missing values\n",
    "critical_cols = [\"transaction_id\",\"date\",\"customer_id\",\"product_category\",\"quantity\",\"price_per_unit\"]\n",
    "present_critical = [c for c in critical_cols if c in df.columns]\n",
    "rows_before_dropna = len(df)\n",
    "df = df.dropna(subset=present_critical)\n",
    "report[\"rows_dropped_missing_critical\"] = rows_before_dropna - len(df)\n",
    "\n",
    "# Fill in any missing ages with median value\n",
    "if \"age\" in df.columns:\n",
    "    age_na_before = df[\"age\"].isna().sum()\n",
    "    df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "    age_na_after = df[\"age\"].isna().sum()\n",
    "    report[\"age_filled_n\"] = age_na_before - age_na_after\n",
    "\n",
    "# Removing any duplicates\n",
    "rows_before_dupes = len(df)\n",
    "if \"transaction_id\" in df.columns:df = df.drop_duplicates(subset=[\"transaction_id\"])\n",
    "else:df = df.drop_duplicates()\n",
    "report[\"rows_removed_duplicates\"] = rows_before_dupes - len(df)\n",
    "\n",
    "# Removing any necessary data points and tracking for before and after\n",
    "rows_before = len(df)\n",
    "if \"age\" in df.columns:\n",
    "    df = df[df[\"age\"].between(12, 100)]\n",
    "report[\"rows_removed_bad_age\"] = rows_before - len(df)\n",
    "rows_before = len(df)\n",
    "if \"quantity\" in df.columns:\n",
    "    df = df[df[\"quantity\"] > 0]\n",
    "report[\"rows_removed_bad_quantity\"] = rows_before - len(df)\n",
    "rows_before = len(df)\n",
    "if \"price_per_unit\" in df.columns:\n",
    "    df = df[df[\"price_per_unit\"] > 0]\n",
    "report[\"rows_removed_bad_price_per_unit\"] = rows_before - len(df)\n",
    "\n",
    "# Fixing total amount calculation\n",
    "if set([\"quantity\", \"price_per_unit\", \"total_amount\"]).issubset(df.columns):\n",
    "    mismatches_before = (df[\"total_amount\"] != (df[\"quantity\"] * df[\"price_per_unit\"])).sum()\n",
    "    df[\"total_amount\"] = df[\"quantity\"] * df[\"price_per_unit\"]\n",
    "    mismatches_after = (df[\"total_amount\"] != (df[\"quantity\"] * df[\"price_per_unit\"])).sum()\n",
    "    report[\"total_amount_mismatches_before_fix\"] = int(mismatches_before)\n",
    "    report[\"total_amount_mismatches_after_fix\"] = int(mismatches_after)\n",
    "\n",
    "# Prep Cleaning for visualization work\n",
    "if \"date\" in df.columns:\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"month_name\"] = df[\"date\"].dt.strftime(\"%b\")\n",
    "    df[\"weekday\"] = df[\"date\"].dt.day_name()\n",
    "for col in [\"gender\", \"product_category\", \"month_name\", \"weekday\"]:\n",
    "    if col in df.columns:df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Creating after summary of cleaned data\n",
    "summarize_df(df, label=\"AFTER CLEANING\")\n",
    "\n",
    "# Printing cleaning report with the before and after metrics\n",
    "report[\"final_rows\"] = len(df)\n",
    "report[\"final_cols\"] = df.shape[1]\n",
    "report[\"rows_removed_total\"] = report[\"start_rows\"] - report[\"final_rows\"]\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLEANING REPORT (WHAT CHANGED)\")\n",
    "print(\"=\" * 70)\n",
    "for k, v in report.items():print(f\"{k}: {v}\")\n",
    "\n",
    "# Saving cleaned data\n",
    "OUTPUT_PATH = r\"C:\\Users\\Mary\\D6_Retail-Sales-Dataset\\data\\processed\\retail_sales_dataset_cleaned.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"\\nCleaned file saved to:\")\n",
    "print(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D6_Retail-Sales-Dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
